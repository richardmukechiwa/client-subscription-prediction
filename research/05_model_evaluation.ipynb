{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b14ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6587d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\client-subscription-prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3c363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f34fbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\client-subscription-prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b61e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_link = 'c:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\client-subscription-prediction'\n",
    "os.chdir(proj_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2527993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir : Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    preprocessor_path : Path\n",
    "    all_params : dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n",
    "    xgb_encoder: Path\n",
    "    sm_model: Path\n",
    "    rf_model: Path\n",
    "    rf_processor: Path\n",
    "    xgb_processor: Path\n",
    "    xgb_model: Path\n",
    "    xgb_final_model: Path\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa6c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clientClassifier.constants import *    \n",
    "from clientClassifier.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccda23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config['artifacts_root']])\n",
    "        \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config['model_evaluation']\n",
    "        params = self.params['xgb_classifier_best']\n",
    "        schema = self.schema['TARGET_COLUMN']\n",
    "        \n",
    "        create_directories([config['root_dir']])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config['root_dir']),\n",
    "            test_data_path=Path(config['test_data_path']),\n",
    "            model_path=Path(config['model_path']),\n",
    "            rf_model = Path(config['rf_model']),\n",
    "            rf_processor  = Path(config['rf_processor']),\n",
    "            preprocessor_path = Path(config['preprocessor_path']),\n",
    "            xgb_encoder= Path(config['xgb_encoder']),\n",
    "            xgb_model= Path(config['xgb_model']),   \n",
    "            sm_model = Path(config['sm_model']),\n",
    "            xgb_processor = Path(config['xgb_processor']),  \n",
    "            all_params=params,\n",
    "            metric_file_name=Path(config['metric_file_name']),\n",
    "            xgb_final_model=Path(config['xgb_final_model']),\n",
    "            target_column=schema.name,\n",
    "            mlflow_uri=\"https://dagshub.com/richardmukechiwa/client-subscription-prediction.mlflow\"\n",
    "        )\n",
    "        \n",
    "        return model_evaluation_config\n",
    "                \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c868b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "import mlflow \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import shap\n",
    "from clientClassifier import logger\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred, average='weighted')\n",
    "        recall = recall_score(actual, pred, average='weighted')\n",
    "        f1 = f1_score(actual, pred, average='weighted')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def log_confusion_matrix(self, actual, predicted, class_names):\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        temp_img_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n",
    "        plt.savefig(temp_img_path)\n",
    "        plt.close()\n",
    "\n",
    "        mlflow.log_artifact(temp_img_path, artifact_path=\"confusion_matrix\")\n",
    "\n",
    "    def log_classification_report(self, actual, predicted, class_names):\n",
    "        report = classification_report(actual, predicted, target_names=class_names)\n",
    "        temp_txt_path = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False).name\n",
    "        with open(temp_txt_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(temp_txt_path, artifact_path=\"xgb_classification_report\")\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        \n",
    "        model = joblib.load(self.config.xgb_final_model)\n",
    "        processor = joblib.load(self.config.xgb_processor)\n",
    "        encoder = joblib.load(self.config.xgb_encoder)\n",
    "    \n",
    "\n",
    "        test_x = test_data.drop(self.config.target_column, axis=1)\n",
    "        test_y = test_data[self.config.target_column]\n",
    "        \n",
    "        #label encoding the target variable\n",
    "        test_y = encoder.transform(test_y)\n",
    "        \n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        mlflow.set_experiment(\"classification_with_xgbclassifier_artifacts_stored\")\n",
    "        \n",
    "\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run()\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            #processing = model.transform(test_x)\n",
    "            processed = processor.transform(test_x)\n",
    "            predicted_qualities = model.predict(processed)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            accuracy, precision, recall, f1 = self.eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            model_name = \"xgb_classifier\" \n",
    "\n",
    "            scores = {\n",
    "                \"model_name\": model_name,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1\n",
    "            }\n",
    "\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "            class_names =encoder.classes_\n",
    "            self.log_confusion_matrix(test_y, predicted_qualities, class_names)\n",
    "            self.log_classification_report(test_y, predicted_qualities, class_names)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"XGB ClassificationModel\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def feature_importance(self):\n",
    "        \n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        \n",
    "        model = joblib.load(self.config.xgb_model)\n",
    "        processor = joblib.load(self.config.xgb_processor)\n",
    "        \n",
    "        test_x = test_data.drop(self.config.target_column, axis=1)\n",
    "        \n",
    "        processed = processor.transform(test_x)\n",
    "        \n",
    "        # getting the feature names from the processor\n",
    "        feature_names = processor.get_feature_names_out()\n",
    "        # Convert the processed data back to a DataFrame with feature names\n",
    "        processed = pd.DataFrame(processed, columns=feature_names)              \n",
    "        \n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "\n",
    "        # Use evaluation/test set ideally\n",
    "        shap_values = explainer.shap_values(processed)\n",
    "\n",
    "        # Plot SHAP summary\n",
    "        shap.summary_plot(shap_values, processed)\n",
    "        \n",
    "        \n",
    "        #findings of the shap summary plot\n",
    "        \n",
    "        logger.info(\"The SHAP analysis revealed that the top five features driving model predictions are:\")\n",
    "        print(\"  \" * 50)\n",
    "        logger.info(\"1. Previous Campaign Outcome (cat__poutcome_success): Clients who previously responded positively to campaigns are much more likely to subscribe again.\")\n",
    "        print(\" \" * 50)\n",
    "        logger.info(\"2. Account Balance (num__balance): Clients with higher account balances are significantly more likely to subscribe.\") \n",
    "        print(\" \" * 50)\n",
    "        logger.info(\"3. Day of Contact (num__day): The specific day of the month when a client is contacted strongly influences the outcome.\")\n",
    "        print(\" \" * 50)\n",
    "        logger.info(\"4. Month of Contact (num__month): Seasonality effects are evident, with specific months (e.g., holiday or bonus periods) boosting subscription likelihood.\")\n",
    "        print(\" \" * 50)\n",
    "        logger.info(\"5. Age of Client (num__age): Age plays a significant role, with different age groups showing distinct patterns in subscription behavior.\")                 \n",
    "\n",
    "    \n",
    "        \n",
    "       \n",
    "    #Evaluation and mlflow  on selected features\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred, average='weighted')\n",
    "        recall = recall_score(actual, pred, average='weighted')\n",
    "        f1 = f1_score(actual, pred, average='weighted')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def log_confusion_matrix(self, actual, predicted, class_names):\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        temp_img_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n",
    "        plt.savefig(temp_img_path)\n",
    "        plt.close()\n",
    "\n",
    "        mlflow.log_artifact(temp_img_path, artifact_path=\"confusion_matrix\")\n",
    "\n",
    "    def log_classification_report(self, actual, predicted, class_names):\n",
    "        report = classification_report(actual, predicted, target_names=class_names)\n",
    "        temp_txt_path = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False).name\n",
    "        with open(temp_txt_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(temp_txt_path, artifact_path=\"xgb_classification_report\")\n",
    "\n",
    "    def log_selected_features_into_mlflow(self):\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        \n",
    "        model = joblib.load(self.config.xgb_final_model)\n",
    "        encoder = joblib.load(self.config.xgb_encoder)\n",
    "    \n",
    "\n",
    "        test_x = test_data.drop(self.config.target_column, axis=1)\n",
    "        test_y = test_data[self.config.target_column]\n",
    "        \n",
    "        \n",
    "        #introduce selected features\n",
    "        important_features = ['age', 'month', 'day', 'balance', 'poutcome']\n",
    "        \n",
    "        test_x  = test_x[important_features]\n",
    "        \n",
    "        #label encoding the target variable\n",
    "        test_y = encoder.transform(test_y)\n",
    "        \n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        mlflow.set_experiment(\"classification_with_xgbclassifier_on_selected_features\")\n",
    "        \n",
    "\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run()\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            #processing = model.transform(test_x)\n",
    "           \n",
    "            predicted_qualities = model.predict(test_x)\n",
    "            \n",
    "            \n",
    "            accuracy, precision, recall, f1 = self.eval_metrics(test_y, predicted_qualities)\n",
    "            \n",
    "            # evaluate the model\n",
    "            xgb_report = classification_report(test_y, predicted_qualities)\n",
    "            xgb_cm = confusion_matrix(test_y, predicted_qualities)   \n",
    "            xgb_accuracy = accuracy_score(test_y, predicted_qualities)   \n",
    "            \n",
    "            #create Confusion Matrix Display\n",
    "            cm_display = ConfusionMatrixDisplay(confusion_matrix=xgb_cm, display_labels=encoder.classes_)\n",
    "            cm_display.plot()\n",
    "            plt.title(\"XGBClassifier Matrix\")\n",
    "\n",
    "            logger.info(f\"XGBoost Classification Report:\\n{xgb_report}\")\n",
    "            logger.info(f\"XGBoost Confusion Matrix:\\n{xgb_cm}\") \n",
    "            logger.info(f\"XGBoost Accuracy: {xgb_accuracy}\")\n",
    "            \n",
    "            \n",
    "           \n",
    "\n",
    "            model_name = \"xgb_classifier_best\" \n",
    "\n",
    "            scores = {\n",
    "                \"model_name\": model_name,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1\n",
    "            }\n",
    "\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "            class_names =encoder.classes_\n",
    "            self.log_confusion_matrix(test_y, predicted_qualities, class_names)\n",
    "            self.log_classification_report(test_y, predicted_qualities, class_names)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"XGB ClassificationModel\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "                \n",
    "   \n",
    "            return xgb_report, xgb_cm, xgb_accuracy\n",
    "        \n",
    "        \n",
    "            \n",
    "          \n",
    "        \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abde84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-03 14:52:51,265: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-03 14:52:51,271: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-03 14:52:51,276: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-03 14:52:51,278: INFO: common: created directory at: artifacts]\n",
      "[2025-05-03 14:52:51,280: INFO: common: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     model_evaluation\u001b[38;5;241m.\u001b[39mlog_selected_features_into_mlflow()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_evaluation_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_evaluation_config()\n\u001b[0;32m      4\u001b[0m     model_evaluation \u001b[38;5;241m=\u001b[39m ModelEvaluation(config\u001b[38;5;241m=\u001b[39mmodel_evaluation_config)  \u001b[38;5;66;03m# Initialize model_evaluation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel_evaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_selected_features_into_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[10], line 197\u001b[0m, in \u001b[0;36mModelEvaluation.log_selected_features_into_mlflow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m#processing = model.transform(test_x)\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(test_x)\n\u001b[0;32m    198\u001b[0m     predicted_qualities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(processed)\n\u001b[0;32m    201\u001b[0m     accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metrics(test_y, predicted_qualities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)  # Initialize model_evaluation\n",
    "    model_evaluation.log_selected_features_into_mlflow()\n",
    "    \n",
    " \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cefb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f853de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb149b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
